{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses Milvus through Docker Compose so you must have Docker installed to run this notebook (Milvus is spun up via `docker compose up -d` as shown in the block below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qU pymilvus langchain sentence-transformers tiktoken octoai-sdk openai\n",
    "# docker-compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OCTOAI_TOKEN\"] = os.getenv(\"OCTOAI_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! model is not default parameter.\n",
      "                model was transferred to model_kwargs.\n",
      "                Please confirm that model is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.octoai_endpoint import OctoAIEndpoint\n",
    "llm = OctoAIEndpoint(\n",
    "        model=\"mixtral-8x7b-instruct-fp16\",\n",
    "        max_tokens=200,\n",
    "        presence_penalty=0,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OctoAIEmbeddings\n",
    "from langchain_community.vectorstores import Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OctoAIEmbeddings(endpoint_url=\"https://text.octoai.run/v1/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = os.listdir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['2_sanitized.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(f\"../task_data/{file}\") as f:\n",
    "        file_text = f.read()\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=256, chunk_overlap=64, \n",
    "    )\n",
    "    texts = text_splitter.split_text(file_text)\n",
    "    \n",
    "    for i, chunked_text in enumerate(texts):\n",
    "        file_texts.append(Document(page_content=chunked_text, \n",
    "                metadata={\"doc_title\": file.split(\".\")[0], \"chunk_num\": i}))\n",
    "        \n",
    "    with open(f\"../task_data/output_{file.split('.')[0]}.txt\", 'w') as output_file:\n",
    "        for document in file_texts:\n",
    "            output_file.write(f\"Title: {document.metadata['doc_title']}, Chunk: {document.metadata['chunk_num']}\\n\")\n",
    "            output_file.write(f\"{document.page_content}\\n\")\n",
    "            output_file.write(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first run\n",
    "# \n",
    "# print(file_texts)\n",
    "# vector_store = Milvus.from_documents(\n",
    "#     file_texts,\n",
    "#     embedding=embeddings,\n",
    "#     connection_args={\"host\": \"localhost\", \"port\": 19530},\n",
    "#     collection_name=\"tasks\"\n",
    "# )\n",
    "\n",
    "# if you already have the data you need stored in Milvus\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"host\": \"localhost\", \"port\": 19530},\n",
    "    collection_name=\"tasks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template=\"\"\"You will receive data in following format: date isfinished content. anwer the question based on these data. For example, 2023-04-02 Y Kayaking on the river means I plan to kayak on 2023-04-02 and I make it (since it is Y) \n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The data you receive is a list of documents, where each document is a series of strings in a specific format. This format consists of a date, an 'N' or 'Y' indicating whether the activity was completed or not, and a description of the activity. The length of this data is 5 documents, each with 11 such strings.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"what is the data you receive, how long it is\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I\\'m sorry, I don\\'t see any connection between your statement \"I also kyaka in 4/4, 4/5...\" and the provided context. Could you please clarify or rephrase your question?'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
